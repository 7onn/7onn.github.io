<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üìúüßô‚Äç‚ôÇÔ∏è tom's blog</title><link>https://blog.7onn.dev/</link><description>Recent content on üìúüßô‚Äç‚ôÇÔ∏è tom's blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 05 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.7onn.dev/index.xml" rel="self" type="application/rss+xml"/><item><title>Emergency response</title><link>https://blog.7onn.dev/post/emergency-response/</link><pubDate>Fri, 21 Jan 2022 09:00:00 -0300</pubDate><author>Tom M G</author><guid>https://blog.7onn.dev/post/emergency-response/</guid><description>
&lt;blockquote>
&lt;p>This following content is a transcript from &lt;a href="https://books.google.com.br/books/about/Site_Reliability_Engineering.html?id=tYrPCwAAQBAJ">Site Reliability Engineering&lt;/a> with some, maybe none, personal adjustments.&lt;/p>
&lt;/blockquote>
&lt;p>Reliability is a function of mean time to failure (MTTF) and mean time to repair (MTTR). The most relevant metric in evaluating the effectiveness of emergency response is how quickly the response team can bring the system back to health - that is, the MTTR.&lt;/p>
&lt;p>Humans add latency. Even if a given system experiences more &lt;em>actual&lt;/em> failures, a system that can avoid emergencies that require human intervention will have higher availability than a system that requires hands-on intervention. When humans are necessary, we have found that thinking through and recording the best practices ahead of time in a &amp;ldquo;playbook&amp;rdquo; produces roughly a 3x improvement in MTTR as compared to the strategy of &amp;ldquo;winging it&amp;rdquo;. The hero jack-of-all-trades on-call engineer does work, but the practiced on-call engineer armed with a playbook works &lt;strong>much better&lt;/strong>.&lt;/p></description></item><item><title>Monitoring</title><link>https://blog.7onn.dev/post/monitoring/</link><pubDate>Fri, 14 Jan 2022 09:00:00 -0300</pubDate><author>Tom M G</author><guid>https://blog.7onn.dev/post/monitoring/</guid><description>
&lt;blockquote>
&lt;p>This following content is a transcript from &lt;a href="https://books.google.com.br/books/about/Site_Reliability_Engineering.html?id=tYrPCwAAQBAJ">Site Reliability Engineering&lt;/a> with some, maybe none, personal adjustments.&lt;/p>
&lt;/blockquote>
&lt;p>Monitoring is one of the primary means by which service owners keep track of a system&amp;rsquo;s health and availability. As such, monitoring strategy should be constructed thoughtfully. A classic and common approach to monitoring is to watch for a specific value or condition, and then trigger an alert when that value is exceeded or that condition occurs. However, this type of alerting is not an effective solution: a system that requires a human to read a message and decide whether or not some type of action needs to be taken in response is fundamentally flawed. Monitoring should never require a human to interpret any part of the alerting domain. Instead, software should do the interpreting, and humans should be notified only when they need to take action.&lt;/p>
&lt;p>There are three kinds of valid monitoring output:&lt;/p>
&lt;h2 id="alerts">Alerts&lt;/h2>
&lt;p>Signify that a human needs to take action immediately in response to something that is either happening or about to happen, in order to improve the situation.&lt;/p>
&lt;h2 id="tickets">Tickets&lt;/h2>
&lt;p>Signify that a human needs to take action, but not immediately. The system can&amp;rsquo;t automatically handle the situation, but if a human takes action in a few days, no damage will result.&lt;/p>
&lt;h2 id="logging">Logging&lt;/h2>
&lt;p>No one needs to look at this information, but it is recorded for diagnostic or forensic puporses. The expectation is that no one reads logs unless something else prompts them to do so.&lt;/p></description></item><item><title>Pursuing maximum change velocity without violating SLO</title><link>https://blog.7onn.dev/post/pursuing-maximum-change-velocity-without-violating-slo/</link><pubDate>Fri, 07 Jan 2022 10:00:00 -0300</pubDate><author>Tom M G</author><guid>https://blog.7onn.dev/post/pursuing-maximum-change-velocity-without-violating-slo/</guid><description>
&lt;blockquote>
&lt;p>This following content is a transcript from &lt;a href="https://books.google.com.br/books/about/Site_Reliability_Engineering.html?id=tYrPCwAAQBAJ">Site Reliability Engineering&lt;/a> with some, maybe none, personal adjustments.&lt;/p>
&lt;/blockquote>
&lt;p>Product development and SRE teams can enjoy a productive working relationship by eliminating the structural conflict in their respective goals. The structural conflict is between the pace of innovation and product stability. This conflict often is expressed indirectly. In SRE we bring this conflict to the fore, and then resolve it with the introduction of an &lt;code>error budget&lt;/code>.&lt;/p>
&lt;p>The error budget stems from the observation that 100% is the &lt;em>wrong reliability target for almost everything&lt;/em>. In general, for any software service or system, 100% is not the right reliability target because no user can tell the difference between a system being 100% available and 99.999% available. There are many other systems in the path between user and service (their laptop, their WiFi, their Internet service provider, the power grid&amp;hellip;) and those systems collectively are far less than 99.999% available. Thus, the marginal difference between 99.999% and 100% gets lost in the noise of other unavailability, and the user receives no benefit from the enormous effort required to add that last 0.001% of availability.&lt;/p>
&lt;p>If 100% us the wrong reliability target for a system, what, then, is the right reliability target for the system? This actually isn&amp;rsquo;t a technical question at all - it&amp;rsquo;s a product question, which should take the following considerations into account:&lt;/p>
&lt;ul>
&lt;li>What level of availability will the users be happy with, given how they use the product?&lt;/li>
&lt;li>What alternatives are available to users who are dissatisfied with the product&amp;rsquo;s availability?&lt;/li>
&lt;li>What happens to users' product usage at different availability levels?&lt;/li>
&lt;/ul>
&lt;p>The business or the product must establish the system&amp;rsquo;s availability target. Once that target is set, the error budget is &lt;code>100 - $AVAILABILITY_TARGET&lt;/code>. A service that&amp;rsquo;s 99.99% available is 0.01% unavailable. That permitted 0.01% unavailability is the service&amp;rsquo;s &lt;em>error budget&lt;/em>. We might spend the budget on anything we want, as long as we don&amp;rsquo;t overspend it.&lt;/p>
&lt;p>So how do we want to spend the error budget? The development team wants to launch features and attract new users. Ideally, we would spend all of our error budget taking risks to launch features quickly. This basic premise describes the whole model of error budgets. As soon as we conceptualize SRE activities in this framework, freeing up the error budget through tactics such as phased rollouts and 1% experiments can optimize for quicker launches.&lt;/p>
&lt;p>The use of an error budget resolves the structural conflict of incentives between development teams and SRE. SRE&amp;rsquo;s goal is no longer &amp;ldquo;zero outages&amp;rdquo;; rather SREs and product developers aim to spend the error budget getting maximum feature velocity. This change makes all the difference. An outage is no longer a &amp;ldquo;bad&amp;rdquo; thing - it is an expected part of the process of innovation and an occurrence that both development and SRE teams manage rather than fear.&lt;/p></description></item><item><title>Ensuring a durable focus on engineering</title><link>https://blog.7onn.dev/post/ensuring-a-durable-focus-on-engineering/</link><pubDate>Sat, 01 Jan 2022 09:00:00 -0300</pubDate><author>Tom M G</author><guid>https://blog.7onn.dev/post/ensuring-a-durable-focus-on-engineering/</guid><description>
&lt;blockquote>
&lt;p>This following content is a transcript from &lt;a href="https://books.google.com.br/books/about/Site_Reliability_Engineering.html?id=tYrPCwAAQBAJ">Site Reliability Engineering&lt;/a> with some, maybe none, personal adjustments.&lt;/p>
&lt;/blockquote>
&lt;p>SREs shouldn&amp;rsquo;t dedicate more than 50% of their time in operational tasks.&lt;/p>
&lt;p>That being said, their remaining time should be spent using their coding skills on project work. In practice, this is accomplished by monitoring the amount of operational work being done, and redirecting excess operational work to the product development teams: reassigning bugs and tickets to development managers, [re]integrating developers into on-call pager rotations, and so on. The redirection ends when the operational load drops back to 50% or lower. This also provides an effective feedback mechanism, guiding developers to build systems that don&amp;rsquo;t need manual intervention. This approach works well when the entire organization - SRE and development alike - understands why the safety valve mechanism exists and supports the goal of having no overflow events because the production doesn&amp;rsquo;t generate enough operational load to require it.&lt;/p>
&lt;p>When they are focused on operations work, on average, SREs should receive a maximum of two events per 8-12-hour on-call shift. This target volume gives the on-call engineer enough time to handle the event accurately and quickly, clean up and restore normal service, and then conduct a postmortem. If more than two events occur regularly per on-call shift, problems can&amp;rsquo;t be investigated thoroughly and engineers are sufficiently overwhelmed to prevent them from learning from these events. A scenario of pager fatigue also won&amp;rsquo;t improve with scale. Conversely, if on-call SREs consistently receive fewer than one event per shift, keeping them on point is a waste of their time.&lt;/p>
&lt;p>Postmortems should be written for all significant incidents, regardless of whether or not they paged; postmortems that did not trigger a page are even more valuable, as they likely point to clear monitoring gaps. This investigation should establish what happened in detail, find all root causes of the event, and assign actions to correct the problem or improve how it is addressed next time. Ideally, we should operate under a &lt;code>blame-free postmortem culture&lt;/code> to expose faults and apply engineering to fix these faults rather than avoiding or minimizing them.&lt;/p></description></item></channel></rss>